{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edde562e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T07:28:06.142314Z",
     "start_time": "2022-11-21T07:28:03.212316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "Requirement already satisfied: requests in /Users/lhs/miniforge3/envs/lhs/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Using cached huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/lhs/miniforge3/envs/lhs/lib/python3.8/site-packages (from transformers) (2022.7.25)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/lhs/miniforge3/envs/lhs/lib/python3.8/site-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/lhs/miniforge3/envs/lhs/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lhs/miniforge3/envs/lhs/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.2.tar.gz (359 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Users/lhs/miniforge3/envs/lhs/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/lhs/miniforge3/envs/lhs/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/lhs/miniforge3/envs/lhs/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/lhs/miniforge3/envs/lhs/lib/python3.8/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lhs/miniforge3/envs/lhs/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lhs/miniforge3/envs/lhs/lib/python3.8/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lhs/miniforge3/envs/lhs/lib/python3.8/site-packages (from requests->transformers) (1.26.11)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[51 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.0-arm64-cpython-38\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.0-arm64-cpython-38/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/__init__.py -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.0-arm64-cpython-38/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/models/__init__.py -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.0-arm64-cpython-38/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/decoders/__init__.py -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.0-arm64-cpython-38/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/normalizers/__init__.py -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.0-arm64-cpython-38/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.0-arm64-cpython-38/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/processors/__init__.py -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.0-arm64-cpython-38/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/trainers/__init__.py -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.0-arm64-cpython-38/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/__init__.py -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.0-arm64-cpython-38/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/__init__.py -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/visualizer.py -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/__init__.pyi -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/models/__init__.pyi -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/processors/__init__.pyi -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.macosx-11.0-arm64-cpython-38/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m running build_rust\n",
      "  \u001b[31m   \u001b[0m error: can't find Rust compiler\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m To update pip, run:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     pip install --upgrade pip\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m and then retry package installation.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build tokenizers\n",
      "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6842984",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T07:28:09.451962Z",
     "start_time": "2022-11-21T07:28:09.444146Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ElectraTokenizerFast\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ElectraForQuestionAnswering\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from transformers import ElectraTokenizerFast\n",
    "from transformers import ElectraForQuestionAnswering\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be22188a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T07:22:41.231826Z",
     "start_time": "2022-11-21T07:22:41.201074Z"
    }
   },
   "outputs": [],
   "source": [
    "class QADataset(Dataset):     # 데이터를 input으로 변환해주는 Dataset 클래스를 상속하여, QA(Question Answering) 과제에 맞게 커스터마이징한다\n",
    "    \n",
    "    def __init__ (self, data_dir: str, tokenizer, max_seq_len: int, mode = 'train'):     # Dataset 클래스는 기본적으로 __init__, __len__, __getitem__를 정의해 주어야 한다\n",
    "        self.mode = mode\n",
    "        self.data = json.load(open(data_dir, 'r', encoding='utf8'))\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        if mode == 'test':\n",
    "            self.encodings, self.question_ids = self.preprocess()\n",
    "        else:\n",
    "            self.encodings, self.answers = self.preprocess()\n",
    "        \n",
    "    def __len__(self):     # index를 통해 input을 순차적으로 읽어오기 위해서는 데이터의 길이가 먼저 확인되어야 한다. __len__ 함수는 input의 길이를 반환해주는 함수\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "    def __getitem__(self, index: int):     # input의 길이가 확인되면 index를 통해 데이터를 불러올 수 있다. __getitem__ 함수는 index에 해당하는 input 데이터를 반환해주는 함수\n",
    "        return {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
    "\n",
    "    \n",
    "    def preprocess(self):\n",
    "        contexts, questions, answers, question_ids = self.read_squad()     # SQuAD(Stanford Question Answering Dataset) 형식의 데이터에서 contexts, questions, answers, question_ids를 읽어오는 함수\n",
    "        if self.mode == 'test':\n",
    "            encodings = self.tokenizer(contexts, questions, truncation=True, max_length = self.max_seq_len, padding=True)\n",
    "            return encodings, question_ids\n",
    "        else: # train or val\n",
    "            self.add_end_idx(answers, contexts)     # train.json에는 질문에 대한 답이 context 내에서 시작되는 index인 'answer_srart'만 있기 때문에, 추가로 'answer_end'를 찾아주는 함수\n",
    "            encodings = self.tokenizer(contexts, questions, truncation=True, max_length = self.max_seq_len, padding=True)\n",
    "            self.add_token_positions(encodings, answers)\n",
    "        \n",
    "            return encodings, answers\n",
    "        \n",
    "    \n",
    "    def read_squad(self):     # SQuAD(Stanford Question Answering Dataset) 형식의 데이터에서 contexts, questions, answers, question_ids를 읽어오는 함수\n",
    "        contexts = []\n",
    "        questions = []\n",
    "        question_ids = []\n",
    "        answers = []\n",
    "        \n",
    "        # train - val split\n",
    "        if self.mode == 'train':\n",
    "            self.data['data'] = self.data['data'][:-1*int(len(self.data['data'])*0.1)]\n",
    "        elif self.mode == 'val':\n",
    "            self.data['data'] = self.data['data'][-1*int(len(self.data['data'])*0.1):]\n",
    "        \n",
    "        \n",
    "        till = len(self.data['data'])\n",
    "        \n",
    "\n",
    "        for group in self.data['data'][:till]:\n",
    "            for passage in group['paragraphs']:\n",
    "                context = passage['context']\n",
    "                for qa in passage['qas']:\n",
    "                    question = qa['question']\n",
    "                    if self.mode == 'test':\n",
    "                        contexts.append(context)\n",
    "                        questions.append(question)\n",
    "                        question_ids.append(qa['question_id'])\n",
    "                    else: # train or val\n",
    "                        for ans in qa['answers']:\n",
    "                            contexts.append(context)\n",
    "                            questions.append(question)\n",
    "\n",
    "                            if qa['is_impossible']:\n",
    "                                answers.append({'text':'','answer_start':-1})\n",
    "                            else:\n",
    "                                answers.append(ans)\n",
    "                \n",
    "        # return formatted data lists\n",
    "        return contexts, questions, answers, question_ids\n",
    "    \n",
    "    \n",
    "    def add_end_idx(self, answers, contexts):     # train.json에는 질문에 대한 답이 context 내에서 시작되는 index인 'answer_srart'만 있기 때문에, 추가로 'answer_end'를 찾아주는 함수\n",
    "        for answer, context in zip(answers, contexts):\n",
    "            gold_text = answer['text']\n",
    "            start_idx = answer['answer_start']\n",
    "            end_idx = start_idx + len(gold_text)\n",
    "\n",
    "            # in case the indices are off 1-2 idxs\n",
    "            if context[start_idx:end_idx] == gold_text:\n",
    "                answer['answer_end'] = end_idx\n",
    "            else:\n",
    "                for n in [1, 2]:\n",
    "                    if context[start_idx-n:end_idx-n] == gold_text:\n",
    "                        answer['answer_start'] = start_idx - n\n",
    "                        answer['answer_end'] = end_idx - n\n",
    "                    elif context[start_idx+n:end_idx+n] == gold_text:\n",
    "                        answer['answer_start'] = start_idx + n\n",
    "                        answer['answer_end'] = end_idx + n\n",
    "                        \n",
    "\n",
    "    def add_token_positions(self, encodings, answers):\n",
    "        # should use Fast tokenizer\n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "        for i in range(len(answers)):\n",
    "            if answers[i]['answer_start'] == -1:\n",
    "                # set [CLS] token as answer if is_impossible\n",
    "                start_positions.append(0)\n",
    "                end_positions.append(1)\n",
    "            else:\n",
    "                start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "\n",
    "                assert 'answer_end' in answers[i].keys(), f'no answer_end at {i}'\n",
    "                end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
    "\n",
    "            # answer passage truncated\n",
    "            if start_positions[-1] is None:\n",
    "                start_positions[-1] = tokenizer.model_max_length                \n",
    "            # end position cannot be found, shift until found\n",
    "            shift = 1\n",
    "            while end_positions[-1] is None:\n",
    "                end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] - shift)\n",
    "                shift += 1\n",
    "                \n",
    "        # char-based -> token based\n",
    "        encodings.update({'start_positions': start_positions, 'end_positions': end_positions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95a65b5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T07:24:53.769994Z",
     "start_time": "2022-11-21T07:24:53.737666Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# torch.utils.data.Dataset : 데이터를 input으로 변환\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m QADataset(data_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/lhs/Desktop/GitHub/AIConnect_YDS_1/NLP_MRC/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAI_train.json\u001b[39m\u001b[38;5;124m'\u001b[39m), tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m, max_seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m, mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m QADataset(data_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/lhs/Desktop/GitHub/AIConnect_YDS_1/NLP_MRC/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAI_val.json\u001b[39m\u001b[38;5;124m'\u001b[39m), tokenizer \u001b[38;5;241m=\u001b[39m tokenizer, max_seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m, mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# torch.utils.data.DataLoader : input을 배치 단위로 리턴해주는 기능\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# torch.utils.data.Dataset : 데이터를 input으로 변환\n",
    "train_dataset = QADataset(data_dir=os.path.join('/Users/lhs/Desktop/GitHub/AIConnect_YDS_1/NLP_MRC/', 'AI_train.json'), tokenizer = tokenizer, max_seq_len = 512, mode = 'train')\n",
    "val_dataset = QADataset(data_dir=os.path.join('/Users/lhs/Desktop/GitHub/AIConnect_YDS_1/NLP_MRC/', 'AI_val.json'), tokenizer = tokenizer, max_seq_len = 512, mode = 'val')\n",
    "\n",
    "# torch.utils.data.DataLoader : input을 배치 단위로 리턴해주는 기능\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              shuffle=True,\n",
    "                              pin_memory=PIN_MEMORY,\n",
    "                              drop_last=DROP_LAST)\n",
    "\n",
    "val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            shuffle=True,\n",
    "                            pin_memory=PIN_MEMORY,\n",
    "                            drop_last=DROP_LAST)\n",
    "\n",
    "print(f\"Load data, train:{len(train_dataset)} val:{len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3209ca13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

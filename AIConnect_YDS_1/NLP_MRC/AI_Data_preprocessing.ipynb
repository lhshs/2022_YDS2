{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d0cf9f-03f5-4446-be51-a96049fa0121",
   "metadata": {
    "id": "74d0cf9f-03f5-4446-be51-a96049fa0121"
   },
   "source": [
    "# 문서 검색 효율화를 위한 기계독해\n",
    "- 1차 모의경진대회(22.11.14 ~ 22.11.25)\n",
    "- 자연어 기계독해(Machine Reading Comprehension) 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NWNHSk-pSskv",
   "metadata": {
    "id": "NWNHSk-pSskv"
   },
   "source": [
    "## 데이터 구조\n",
    "\n",
    "```\n",
    "$ MRC/\n",
    "├── DATA/\n",
    "│   ├── train.json\n",
    "│   ├── test.json\n",
    "│   └── sample_submission.csv\n",
    "├── prediction.csv (코드 실행 후 생성)\n",
    "├── results/ (코드 실행 후 생성)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xidLcL3yK3wm",
   "metadata": {
    "id": "xidLcL3yK3wm"
   },
   "source": [
    "#0. 사전 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qPmVS-p6K992",
   "metadata": {
    "id": "qPmVS-p6K992"
   },
   "source": [
    "##0.1 구글 드라이브 마운트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bYuiNxj7LQgH",
   "metadata": {
    "id": "bYuiNxj7LQgH"
   },
   "source": [
    "##0.2 라이브러리 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420e527f-ff52-4bb3-a260-565fe2f91e2d",
   "metadata": {
    "id": "420e527f-ff52-4bb3-a260-565fe2f91e2d"
   },
   "source": [
    "##1. 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b80416a-1082-4ae9-8057-18eb163dfc15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T07:54:38.073443Z",
     "start_time": "2022-11-21T07:54:20.012671Z"
    },
    "id": "5b80416a-1082-4ae9-8057-18eb163dfc15"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "from transformers import ElectraTokenizerFast\n",
    "from transformers import ElectraForQuestionAnswering\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e031838b-d5ad-4887-ae60-07ff98a71a0a",
   "metadata": {
    "id": "e031838b-d5ad-4887-ae60-07ff98a71a0a"
   },
   "source": [
    "##2. 하이퍼파라미터 및 기타 인자 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7be91e-9928-453e-8aeb-a6af032b7dcb",
   "metadata": {
    "id": "9e7be91e-9928-453e-8aeb-a6af032b7dcb"
   },
   "source": [
    "###2.1 데이터 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2bee580a-4c2a-42b4-9f1d-78b8d5e63981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T07:59:10.918488Z",
     "start_time": "2022-11-21T07:59:10.914797Z"
    },
    "id": "2bee580a-4c2a-42b4-9f1d-78b8d5e63981"
   },
   "outputs": [],
   "source": [
    "PROJECT_DIR = '/Users/lhs/Desktop/GitHub/AIConnect_YDS_1' # 프로젝트 디렉토리 설정\n",
    "DATA_DIR= '/Users/lhs/Desktop/GitHub/AIConnect_YDS_1/NLP_MRC' # 데이터 디렉토리 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94e33422",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f923389a-631d-4a17-8542-382fa0d6e68b",
   "metadata": {
    "id": "f923389a-631d-4a17-8542-382fa0d6e68b"
   },
   "source": [
    "###2.2 시드 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "97cb0fc4-cce7-45b7-876d-84d00a32f68c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T07:55:53.821004Z",
     "start_time": "2022-11-21T07:55:53.814334Z"
    },
    "id": "97cb0fc4-cce7-45b7-876d-84d00a32f68c"
   },
   "outputs": [],
   "source": [
    "# 난수 생성기가 항상 일정한 값을 출력하게 하기 위해 seed 고정\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88051bcc-8a4e-4552-af85-4fb62d9e2658",
   "metadata": {
    "id": "88051bcc-8a4e-4552-af85-4fb62d9e2658"
   },
   "source": [
    "###2.3 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "42d42463-8bf9-4ea7-a42a-72f763adb515",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T07:55:55.323244Z",
     "start_time": "2022-11-21T07:55:55.318798Z"
    },
    "id": "42d42463-8bf9-4ea7-a42a-72f763adb515"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 3.0e-4     # 학습률(learning rate)은 경사하강법(gradient descent)을 통해 내리막길을 내려갈 때의 보폭\n",
    "BATCH_SIZE = 30    # 배치(batch)는 모델의 가중치(weights)를 업데이트하는 학습 데이터의 단위. 여기서는 16개를 학습할 때마다 모델의 가중치(weights)를 업데이트한다는 것\n",
    "PIN_MEMORY = True\n",
    "NUM_WORKERS = 0\n",
    "EPOCHS = 3     # 에폭은 전체 학습 데이터를 학습에 사용하는 횟수. 주어진 학습 데이터를 여러번 학습할 수 있음\n",
    "DROP_LAST = False\n",
    "EARLY_STOPPING_MODE = min\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "EARLY_STOPPING_TARGET = 'val_loss'     # validation set의 loss를 기준으로 early_stopping 여부를 결정할 것\n",
    "LOGGING_INTERVAL = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a3191b-b482-4f9f-85e0-be96c5f09824",
   "metadata": {
    "id": "01a3191b-b482-4f9f-85e0-be96c5f09824"
   },
   "source": [
    "###2.4 디바이스 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863de192-3f18-421f-a06f-a5964bdbeb87",
   "metadata": {
    "id": "863de192-3f18-421f-a06f-a5964bdbeb87"
   },
   "source": [
    "##3. Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c5fa64b-a355-43d2-95dc-94bb8605cc15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T07:56:07.018455Z",
     "start_time": "2022-11-21T07:56:06.988798Z"
    },
    "id": "2c5fa64b-a355-43d2-95dc-94bb8605cc15"
   },
   "outputs": [],
   "source": [
    "class QADataset(Dataset):     # 데이터를 input으로 변환해주는 Dataset 클래스를 상속하여, QA(Question Answering) 과제에 맞게 커스터마이징한다\n",
    "    \n",
    "    def __init__ (self, data_dir: str, tokenizer, max_seq_len: int, mode = 'train'):     # Dataset 클래스는 기본적으로 __init__, __len__, __getitem__를 정의해 주어야 한다\n",
    "        self.mode = mode\n",
    "        self.data = json.load(open(data_dir, 'r', encoding='utf8'))\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        if mode == 'test':\n",
    "            self.encodings, self.question_ids = self.preprocess()\n",
    "        else:\n",
    "            self.encodings, self.answers = self.preprocess()\n",
    "        \n",
    "    def __len__(self):     # index를 통해 input을 순차적으로 읽어오기 위해서는 데이터의 길이가 먼저 확인되어야 한다. __len__ 함수는 input의 길이를 반환해주는 함수\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "    def __getitem__(self, index: int):     # input의 길이가 확인되면 index를 통해 데이터를 불러올 수 있다. __getitem__ 함수는 index에 해당하는 input 데이터를 반환해주는 함수\n",
    "        return {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
    "\n",
    "    \n",
    "    def preprocess(self):\n",
    "        contexts, questions, answers, question_ids = self.read_squad()     # SQuAD(Stanford Question Answering Dataset) 형식의 데이터에서 contexts, questions, answers, question_ids를 읽어오는 함수\n",
    "        if self.mode == 'test':\n",
    "            encodings = self.tokenizer(contexts, questions, truncation=True, max_length = self.max_seq_len, padding=True)\n",
    "            return encodings, question_ids\n",
    "        else: # train or val\n",
    "            self.add_end_idx(answers, contexts)     # train.json에는 질문에 대한 답이 context 내에서 시작되는 index인 'answer_srart'만 있기 때문에, 추가로 'answer_end'를 찾아주는 함수\n",
    "            encodings = self.tokenizer(contexts, questions, truncation=True, max_length = self.max_seq_len, padding=True)\n",
    "            self.add_token_positions(encodings, answers)\n",
    "        \n",
    "            return encodings, answers\n",
    "        \n",
    "    \n",
    "    def read_squad(self):     # SQuAD(Stanford Question Answering Dataset) 형식의 데이터에서 contexts, questions, answers, question_ids를 읽어오는 함수\n",
    "        contexts = []\n",
    "        questions = []\n",
    "        question_ids = []\n",
    "        answers = []\n",
    "        \n",
    "        # train - val split\n",
    "        if self.mode == 'train':\n",
    "            self.data['data'] = self.data['data'][:-1*int(len(self.data['data'])*0.1)]\n",
    "        elif self.mode == 'val':\n",
    "            self.data['data'] = self.data['data'][-1*int(len(self.data['data'])*0.1):]\n",
    "        \n",
    "        \n",
    "        till = len(self.data['data'])\n",
    "        \n",
    "\n",
    "        for group in self.data['data'][:till]:\n",
    "            for passage in group['paragraphs']:\n",
    "                context = passage['context']\n",
    "                for qa in passage['qas']:\n",
    "                    question = qa['question']\n",
    "                    if self.mode == 'test':\n",
    "                        contexts.append(context)\n",
    "                        questions.append(question)\n",
    "                        question_ids.append(qa['question_id'])\n",
    "                    else: # train or val\n",
    "                        for ans in qa['answers']:\n",
    "                            contexts.append(context)\n",
    "                            questions.append(question)\n",
    "\n",
    "                            if qa['is_impossible']:\n",
    "                                answers.append({'text':'','answer_start':-1})\n",
    "                            else:\n",
    "                                answers.append(ans)\n",
    "                \n",
    "        # return formatted data lists\n",
    "        return contexts, questions, answers, question_ids\n",
    "    \n",
    "    \n",
    "    def add_end_idx(self, answers, contexts):     # train.json에는 질문에 대한 답이 context 내에서 시작되는 index인 'answer_srart'만 있기 때문에, 추가로 'answer_end'를 찾아주는 함수\n",
    "        for answer, context in zip(answers, contexts):\n",
    "            gold_text = answer['text']\n",
    "            start_idx = answer['answer_start']\n",
    "            end_idx = start_idx + len(gold_text)\n",
    "\n",
    "            # in case the indices are off 1-2 idxs\n",
    "            if context[start_idx:end_idx] == gold_text:\n",
    "                answer['answer_end'] = end_idx\n",
    "            else:\n",
    "                for n in [1, 2]:\n",
    "                    if context[start_idx-n:end_idx-n] == gold_text:\n",
    "                        answer['answer_start'] = start_idx - n\n",
    "                        answer['answer_end'] = end_idx - n\n",
    "                    elif context[start_idx+n:end_idx+n] == gold_text:\n",
    "                        answer['answer_start'] = start_idx + n\n",
    "                        answer['answer_end'] = end_idx + n\n",
    "                        \n",
    "\n",
    "    def add_token_positions(self, encodings, answers):\n",
    "        # should use Fast tokenizer\n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "        for i in range(len(answers)):\n",
    "            if answers[i]['answer_start'] == -1:\n",
    "                # set [CLS] token as answer if is_impossible\n",
    "                start_positions.append(0)\n",
    "                end_positions.append(1)\n",
    "            else:\n",
    "                start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "\n",
    "                assert 'answer_end' in answers[i].keys(), f'no answer_end at {i}'\n",
    "                end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
    "\n",
    "            # answer passage truncated\n",
    "            if start_positions[-1] is None:\n",
    "                start_positions[-1] = tokenizer.model_max_length                \n",
    "            # end position cannot be found, shift until found\n",
    "            shift = 1\n",
    "            while end_positions[-1] is None:\n",
    "                end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] - shift)\n",
    "                shift += 1\n",
    "                \n",
    "        # char-based -> token based\n",
    "        encodings.update({'start_positions': start_positions, 'end_positions': end_positions})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5842a98-c4d9-477c-bfbb-db93a1bdca32",
   "metadata": {
    "id": "c5842a98-c4d9-477c-bfbb-db93a1bdca32"
   },
   "source": [
    "##4. 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "69904ef0-ec62-4eb2-8540-9f44a112fc1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T07:56:43.778645Z",
     "start_time": "2022-11-21T07:56:43.772388Z"
    },
    "id": "69904ef0-ec62-4eb2-8540-9f44a112fc1c"
   },
   "outputs": [],
   "source": [
    "class electra(nn.Module):     # pytorch의 모든 neural network 모델들은 torch.nn.Module 클래스를 상속해야 한다. 기본적으로 __init__()과 forward 함수가 override(재정의)되어야 하며, forward 함수는 모델의 계산을 실행하는 것을 뜻한다.\n",
    "\n",
    "    def __init__(self, pretrained, **kwargs):\n",
    "        super(electra, self).__init__()\n",
    "\n",
    "        self.model = ElectraForQuestionAnswering.from_pretrained(pretrained)     # Hugging Face에서 pretrain된 모델을 가져와서 model 변수에 저장한다.\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, start_positions=None, end_positions=None):\n",
    "        \n",
    "        outputs = self.model(input_ids = input_ids, \n",
    "                             attention_mask = attention_mask,\n",
    "                             start_positions = start_positions,\n",
    "                             end_positions = end_positions)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9615d17d-1f80-45e9-8583-e4b3f3a84f44",
   "metadata": {
    "id": "9615d17d-1f80-45e9-8583-e4b3f3a84f44"
   },
   "source": [
    "##5. Utils 정의\n",
    "###5.1 EarlyStopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e9ea7473-7a39-48d9-a0c7-5ef197c99547",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T07:57:34.141064Z",
     "start_time": "2022-11-21T07:57:34.128006Z"
    },
    "id": "e9ea7473-7a39-48d9-a0c7-5ef197c99547"
   },
   "outputs": [],
   "source": [
    "class EarlyStopper():     # 일정 기간 모델 성능에 개선이 없으면, 학습을 중단하는 기능\n",
    "\n",
    "    def __init__(self, patience: int, mode:str)-> None:\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "\n",
    "        # Initiate\n",
    "        self.patience_counter = 0\n",
    "        self.stop = False\n",
    "        self.best_loss = np.inf\n",
    "\n",
    "        print(f\"Initiated early stopper, mode: {self.mode}, best score: {self.best_loss}, patience: {self.patience}\")\n",
    "\n",
    "        \n",
    "    def check_early_stopping(self, loss: float)-> None:\n",
    "        loss = -loss if self.mode == 'max' else loss  # get max value if mode set to max\n",
    "\n",
    "        if loss > self.best_loss:\n",
    "            # got worse score\n",
    "            self.patience_counter += 1\n",
    "\n",
    "            print(f\"Early stopper, counter {self.patience_counter}/{self.patience}, best:{abs(self.best_loss)} -> now:{abs(loss)}\")\n",
    "            \n",
    "            if self.patience_counter == self.patience:\n",
    "                print(f\"Early stopper, stop\")\n",
    "                self.stop = True  # end\n",
    "\n",
    "        elif loss <= self.best_loss:\n",
    "            # got better score\n",
    "            self.patience_counter = 0\n",
    "            \n",
    "            print(f\"Early stopper, counter {self.patience_counter}/{self.patience}, best:{abs(self.best_loss)} -> now:{abs(loss)}\")\n",
    "            print(f\"Set counter as {self.patience_counter}\")\n",
    "            print(f\"Update best score as {abs(loss)}\")\n",
    "            \n",
    "            self.best_loss = loss\n",
    "            \n",
    "        else:\n",
    "            print('debug')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53b4cda-5607-4a52-ad4b-a0ae33940004",
   "metadata": {
    "id": "c53b4cda-5607-4a52-ad4b-a0ae33940004"
   },
   "source": [
    "###5.2 Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "636e914f-d8cb-47b4-8fa0-3f6918653ae0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T07:56:11.388253Z",
     "start_time": "2022-11-21T07:56:11.364675Z"
    },
    "id": "636e914f-d8cb-47b4-8fa0-3f6918653ae0"
   },
   "outputs": [],
   "source": [
    "class Trainer():     # 학습을 위한 Trainer 클래스 정의\n",
    "\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 optimizer,\n",
    "                 loss,\n",
    "                 metrics,\n",
    "                 device,\n",
    "                 tokenizer,\n",
    "                 interval=100):\n",
    "        \n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        self.device = device\n",
    "        self.interval = interval\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # History\n",
    "        self.loss_sum = 0  # Epoch loss sum\n",
    "        self.loss_mean = 0 # Epoch loss mean\n",
    "        self.y = list()\n",
    "        self.y_preds = list()\n",
    "        self.score_dict = dict()  # metric score\n",
    "        self.elapsed_time = 0\n",
    "        \n",
    "\n",
    "    def train(self, mode, dataloader, tokenizer, epoch_index=0):\n",
    "        \n",
    "        start_timestamp = time()\n",
    "        self.model.train() if mode == 'train' else self.model.eval()     # 모델을 train(eval) mode로 전환.  train(eval) mode에서는 dropout, batchnorm이 적용된다(적용되지 않는다)\n",
    " \n",
    "        for batch_index, batch in enumerate(tqdm(dataloader, leave=True)):\n",
    "            \n",
    "            self.optimizer.zero_grad()     # 파라미터 업데이트는 batch 단위로 이루어지고, 매 batch마다 이전 스텝에서 계산된 gradient를 초기화해주어야 함\n",
    "            # pull all the tensor batches required for training\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            start_positions = batch['start_positions'].to(self.device)\n",
    "            end_positions = batch['end_positions'].to(self.device)\n",
    "            \n",
    "            # train model on batch and return outputs (incl. loss)\n",
    "            # Inference\n",
    "            outputs = self.model(input_ids, attention_mask=attention_mask,\n",
    "                            start_positions=start_positions,\n",
    "                            end_positions=end_positions)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            start_score = outputs.start_logits\n",
    "            end_score = outputs.end_logits\n",
    "            \n",
    "            \n",
    "            start_idx = torch.argmax(start_score, dim=1).cpu().tolist()\n",
    "            end_idx = torch.argmax(end_score, dim=1).cpu().tolist()\n",
    "            \n",
    "            # Update\n",
    "            if mode == 'train':\n",
    "                loss.backward()     # backpropagation\n",
    "                self.optimizer.step()     # 파라미터 업데이트\n",
    "                \n",
    "            elif mode in ['val', 'test']:\n",
    "                pass\n",
    "            \n",
    "            # History\n",
    "            self.loss_sum += loss.item()\n",
    "            \n",
    "            # create answer; list of strings\n",
    "            for i in range(len(input_ids)):\n",
    "                if start_idx[i] > end_idx[i]:\n",
    "                    output = ''\n",
    "                \n",
    "                self.y_preds.append(self.tokenizer.decode(input_ids[i][start_idx[i]:end_idx[i]]))\n",
    "                self.y.append(self.tokenizer.decode(input_ids[i][start_positions[i]:end_positions[i]]))\n",
    "\n",
    "\n",
    "            # Logging\n",
    "            if batch_index % self.interval == 0:\n",
    "                print(f\"batch: {batch_index}/{len(dataloader)} loss: {loss.item()}\")\n",
    "                \n",
    "        # Epoch history\n",
    "        self.loss_mean = self.loss_sum / len(dataloader)  # Epoch loss mean\n",
    "\n",
    "        # Metric\n",
    "        score = self.metrics(self.y, self.y_preds)\n",
    "        self.score_dict['metric_name'] = score\n",
    "\n",
    "        # Elapsed time\n",
    "        end_timestamp = time()\n",
    "        self.elapsed_time = end_timestamp - start_timestamp\n",
    "\n",
    "    def clear_history(self):\n",
    "        self.loss_sum = 0\n",
    "        self.loss_mean = 0\n",
    "        self.y_preds = list()\n",
    "        self.y = list()\n",
    "        self.score_dict = dict()\n",
    "        self.elapsed_time = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c1c276-2264-43b9-aa62-44cfd81ee395",
   "metadata": {
    "id": "66c1c276-2264-43b9-aa62-44cfd81ee395"
   },
   "source": [
    "##6. 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa925da6-d006-4fb9-a763-021ffe9bc8e4",
   "metadata": {
    "id": "aa925da6-d006-4fb9-a763-021ffe9bc8e4"
   },
   "source": [
    "###6.1 모델과 기타 utils 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "714f8a41-c14f-4fc5-84ae-b04f0452a952",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T07:58:51.296243Z",
     "start_time": "2022-11-21T07:58:49.473296Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "714f8a41-c14f-4fc5-84ae-b04f0452a952",
    "outputId": "bbc0e8b7-13aa-4f87-fdfa-695579705f09"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-small-v3-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = electra(pretrained='monologg/koelectra-small-v3-discriminator').to(device)\n",
    "\n",
    "# Set optimizer, loss function, metric function\n",
    "optimizer = optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "loss = F.cross_entropy\n",
    "metrics = accuracy_score\n",
    "\n",
    "# Set tokenizer\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained('monologg/koelectra-small-v3-discriminator')\n",
    "\n",
    "# Set Trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=metrics,\n",
    "                  device=device,\n",
    "                  tokenizer=tokenizer,\n",
    "                  interval=LOGGING_INTERVAL)\n",
    "\n",
    "# Set earlystopper\n",
    "# early_stopper = EarlyStopper(patience=EARLY_STOPPING_PATIENCE,\n",
    "#                             mode=min)\n",
    "\n",
    "# Set train serial\n",
    "# kst = timezone(timedelta(hours=9))\n",
    "# train_serial = datetime.now(tz=kst).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "# Set recorder \n",
    "# RECORDER_DIR = os.path.join(PROJECT_DIR, 'results', 'train', train_serial)\n",
    "# os.makedirs(RECORDER_DIR, exist_ok=True)\n",
    "\n",
    "# recorder = Recorder(record_dir=RECORDER_DIR,\n",
    "#                     model=model,\n",
    "#                     optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715ceb0-4555-431a-ab9d-8f6f99dd783f",
   "metadata": {
    "id": "c715ceb0-4555-431a-ab9d-8f6f99dd783f"
   },
   "source": [
    "###6.2 Dataset & Dataloader 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4076e47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data, train:106128 \n",
      "Load data, train:25262 \n"
     ]
    }
   ],
   "source": [
    "# torch.utils.data.Dataset : 데이터를 input으로 변환\n",
    "train_dataset = QADataset(data_dir=os.path.join(DATA_DIR, 'AI_train.json'), tokenizer = tokenizer, max_seq_len = 512, mode = 'train')\n",
    "train_dataset2 = QADataset(data_dir=os.path.join(DATA_DIR, 'train.json'), tokenizer = tokenizer, max_seq_len = 512, mode = 'train')\n",
    "\n",
    "print(f\"Load data, train:{len(train_dataset)} \")\n",
    "print(f\"Load data, train:{len(train_dataset2)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "19d9d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = train_dataloader + train_dataloader2\n",
    "train_dataset3 = train_dataset + train_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c8bf275f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data, train:234262 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Load data, train:{len(train_dataset3)} \") # val:{len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d504be1e-67c8-4de5-b8d0-225ac0b8fd65",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-11-21T08:05:52.740Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d504be1e-67c8-4de5-b8d0-225ac0b8fd65",
    "outputId": "8ac571af-dc24-4d04-f07a-771126d94eb0"
   },
   "outputs": [],
   "source": [
    "# torch.utils.data.DataLoader : input을 배치 단위로 리턴해주는 기능\n",
    "train_dataloader = DataLoader(dataset=train_dataset3,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              shuffle=True,\n",
    "                              pin_memory=PIN_MEMORY,\n",
    "                              drop_last=DROP_LAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "edee42e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0xd884b5eb0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a4321c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data, train:2762 \n",
      "Load data, train:23092 \n"
     ]
    }
   ],
   "source": [
    "val_dataset = QADataset(data_dir=os.path.join(DATA_DIR, 'train.json'), tokenizer = tokenizer, max_seq_len = 512, mode = 'val')\n",
    "val_dataset2 = QADataset(data_dir=os.path.join(DATA_DIR, 'AI_train.json'), tokenizer = tokenizer, max_seq_len = 512, mode = 'val')\n",
    "\n",
    "print(f\"Load data, train:{len(val_dataset)} \")\n",
    "print(f\"Load data, train:{len(val_dataset2)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b9fca6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset3 = val_dataset + val_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3319ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data, train:25262 \n"
     ]
    }
   ],
   "source": [
    "val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            shuffle=True,\n",
    "                            pin_memory=PIN_MEMORY,\n",
    "                            drop_last=DROP_LAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "caa19c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e72cf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259524"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8897a39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data, train:23092 \n"
     ]
    }
   ],
   "source": [
    "# torch.utils.data.Dataset : 데이터를 input으로 변환\n",
    "# train_dataset2 = QADataset(data_dir=os.path.join(DATA_DIR, 'train.json'), tokenizer = tokenizer, max_seq_len = 512, mode = 'train')\n",
    "val_dataset = QADataset(data_dir=os.path.join(DATA_DIR, 'AI_train.json'), tokenizer = tokenizer, max_seq_len = 512, mode = 'val')\n",
    "\n",
    "# torch.utils.data.DataLoader : input을 배치 단위로 리턴해주는 기능\n",
    "# train_dataloader = DataLoader(dataset=train_dataset,\n",
    "#                               batch_size=BATCH_SIZE,\n",
    "#                               num_workers=NUM_WORKERS, \n",
    "#                               shuffle=True,\n",
    "#                               pin_memory=PIN_MEMORY,\n",
    "#                               drop_last=DROP_LAST)\n",
    "\n",
    "val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            shuffle=True,\n",
    "                            pin_memory=PIN_MEMORY,\n",
    "                            drop_last=DROP_LAST)\n",
    "\n",
    "print(f\"Load data, train:{len(val_dataset)} \") # val:{len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda7a09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data, train:2762 \n"
     ]
    }
   ],
   "source": [
    "# torch.utils.data.Dataset : 데이터를 input으로 변환\n",
    "# train_dataset2 = QADataset(data_dir=os.path.join(DATA_DIR, 'train.json'), tokenizer = tokenizer, max_seq_len = 512, mode = 'train')\n",
    "val_dataset2 = QADataset(data_dir=os.path.join(DATA_DIR, 'train.json'), tokenizer = tokenizer, max_seq_len = 512, mode = 'val')\n",
    "\n",
    "# torch.utils.data.DataLoader : input을 배치 단위로 리턴해주는 기능\n",
    "# train_dataloader = DataLoader(dataset=train_dataset,\n",
    "#                               batch_size=BATCH_SIZE,\n",
    "#                               num_workers=NUM_WORKERS, \n",
    "#                               shuffle=True,\n",
    "#                               pin_memory=PIN_MEMORY,\n",
    "#                               drop_last=DROP_LAST)\n",
    "\n",
    "val_dataloader = DataLoader(dataset=val_dataset2,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            shuffle=True,\n",
    "                            pin_memory=PIN_MEMORY,\n",
    "                            drop_last=DROP_LAST)\n",
    "\n",
    "print(f\"Load data, train:{len(val_dataset2)} \") # val:{len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f175425",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset3 = val_dataset + val_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753c6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a4e5c0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T08:05:02.631095Z",
     "start_time": "2022-11-21T08:05:02.594948Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   2, 6380,   16,  ...,    0,    0,    0],\n",
       "         [   2, 6380,   16,  ...,    0,    0,    0],\n",
       "         [   2, 6380,   16,  ...,    0,    0,    0],\n",
       "         [   2, 6380,   16,  ...,    0,    0,    0],\n",
       "         [   2, 6347, 6318,  ...,    0,    0,    0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'start_positions': tensor([126, 126, 126,   0, 162]),\n",
       " 'end_positions': tensor([128, 128, 128,   1, 163])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6356e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e685537-7c8b-439c-8258-00c658bb360a",
   "metadata": {
    "id": "3e685537-7c8b-439c-8258-00c658bb360a"
   },
   "source": [
    "###6.3 Epoch 단위 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "661bf57c-20d2-4e14-bffb-e7db7eb85f78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "661bf57c-20d2-4e14-bffb-e7db7eb85f78",
    "outputId": "1c5dba06-de97-44f6-941c-1c1158f2d084"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0/2\n",
      "--Train 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1053 [00:00<07:10,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0/1053 loss: 6.073224067687988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 201/1053 [01:12<05:07,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 200/1053 loss: 0.6699638366699219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 401/1053 [02:24<03:51,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 400/1053 loss: 1.2929861545562744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 601/1053 [03:35<02:41,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 600/1053 loss: 1.456833839416504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 801/1053 [04:47<01:30,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 800/1053 loss: 1.1014485359191895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 1001/1053 [05:59<00:18,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1000/1053 loss: 0.9898725748062134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1053/1053 [06:17<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val 0/2\n",
      "--Val 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/116 [00:00<00:13,  8.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0/116 loss: 1.0423444509506226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [00:15<00:00,  7.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write row 0\n",
      "Early stopper, counter 0/10, best:inf -> now:0.7806744644867962\n",
      "Set counter as 0\n",
      "Update best score as 0.7806744644867962\n",
      "Recorder, epoch 0 Model saved: /content/drive/MyDrive/YDS/AIConnet_YDS_1/NLP_MRC/results/train/20221118_171617/model.pt\n",
      "Train 1/2\n",
      "--Train 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1053 [00:00<06:11,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0/1053 loss: 0.6458122730255127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 201/1053 [01:12<05:03,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 200/1053 loss: 0.6652371883392334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 401/1053 [02:23<03:53,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 400/1053 loss: 0.52034592628479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 601/1053 [03:35<02:41,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 600/1053 loss: 1.0956295728683472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 801/1053 [04:47<01:30,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 800/1053 loss: 0.7724480032920837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 1001/1053 [05:59<00:18,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1000/1053 loss: 0.4822039306163788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1053/1053 [06:17<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val 1/2\n",
      "--Val 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/116 [00:00<00:14,  7.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0/116 loss: 0.6325180530548096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [00:15<00:00,  7.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write row 1\n",
      "Early stopper, counter 0/10, best:0.7806744644867962 -> now:0.7201675145790495\n",
      "Set counter as 0\n",
      "Update best score as 0.7201675145790495\n",
      "Recorder, epoch 1 Model saved: /content/drive/MyDrive/YDS/AIConnet_YDS_1/NLP_MRC/results/train/20221118_171617/model.pt\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for epoch_index in range(EPOCHS):\n",
    "\n",
    "    # Set Recorder row\n",
    "    row_dict = dict()\n",
    "    row_dict['epoch_index'] = epoch_index\n",
    "    row_dict['train_serial'] = train_serial\n",
    "\n",
    "    \"\"\"\n",
    "    Train\n",
    "    \"\"\"\n",
    "    print(f\"Train {epoch_index}/{EPOCHS}\")\n",
    "    print(f\"--Train {epoch_index}/{EPOCHS}\")\n",
    "    trainer.train(dataloader=train_dataloader, epoch_index=epoch_index, tokenizer=tokenizer, mode='train')\n",
    "\n",
    "    row_dict['train_loss'] = trainer.loss_mean\n",
    "    row_dict['train_elapsed_time'] = trainer.elapsed_time \n",
    "\n",
    "    for metric_str, score in trainer.score_dict.items():\n",
    "        row_dict[f\"train_{metric_str}\"] = score\n",
    "    trainer.clear_history()\n",
    "\n",
    "    \"\"\"\n",
    "    Validation\n",
    "    \"\"\"\n",
    "    print(f\"Val {epoch_index}/{EPOCHS}\")\n",
    "    print(f\"--Val {epoch_index}/{EPOCHS}\")\n",
    "    trainer.train(dataloader=val_dataloader, epoch_index=epoch_index, tokenizer=tokenizer, mode='val')\n",
    "\n",
    "    row_dict['val_loss'] = trainer.loss_mean\n",
    "    row_dict['val_elapsed_time'] = trainer.elapsed_time \n",
    "\n",
    "    for metric_str, score in trainer.score_dict.items():\n",
    "        row_dict[f\"val_{metric_str}\"] = score\n",
    "    trainer.clear_history()\n",
    "\n",
    "    \"\"\"\n",
    "    Record\n",
    "    \"\"\"\n",
    "    recorder.add_row(row_dict)\n",
    "\n",
    "    \"\"\"\n",
    "    Early stopper\n",
    "    \"\"\"\n",
    "    early_stopping_target = EARLY_STOPPING_TARGET\n",
    "    early_stopper.check_early_stopping(loss=row_dict[early_stopping_target])\n",
    "\n",
    "    if early_stopper.patience_counter == 0:\n",
    "        recorder.save_weight(epoch=epoch_index)\n",
    "        best_row_dict = copy.deepcopy(row_dict)\n",
    "\n",
    "    if early_stopper.stop == True:\n",
    "        print(f\"Early stopped, counter {early_stopper.patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d2987-378c-45df-84bf-ee4589ac61d4",
   "metadata": {
    "id": "ed9d2987-378c-45df-84bf-ee4589ac61d4"
   },
   "source": [
    "##7. 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dee7dae-24d0-4cf3-b641-c3938ceda233",
   "metadata": {
    "id": "0dee7dae-24d0-4cf3-b641-c3938ceda233"
   },
   "source": [
    "###7.1 테스트 Dataset & Dataloader 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a67fcd63-de84-41fe-a620-8771d799722b",
   "metadata": {
    "id": "a67fcd63-de84-41fe-a620-8771d799722b"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "test_dataset = QADataset(data_dir=os.path.join(DATA_DIR, 'test.json'), tokenizer = tokenizer, max_seq_len = 512, mode = 'test')\n",
    "\n",
    "question_ids = test_dataset.question_ids\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            shuffle=False,\n",
    "                            pin_memory=PIN_MEMORY,\n",
    "                            drop_last=DROP_LAST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f8fc5-d1e8-4027-b8c7-ed87ec409246",
   "metadata": {
    "id": "733f8fc5-d1e8-4027-b8c7-ed87ec409246"
   },
   "source": [
    "###7.2 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "86cc6eeb-6f7c-4765-9478-83cb9ce62954",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86cc6eeb-6f7c-4765-9478-83cb9ce62954",
    "outputId": "13a128bc-2411-48e8-ce21-1456d1a1c58b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-small-v3-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "\n",
    "model = electra(pretrained='monologg/koelectra-small-v3-discriminator').to(device)\n",
    "\n",
    "checkpoint = torch.load(os.path.join(RECORDER_DIR, 'model.pt'))\n",
    "\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "_h2ZKq5Tctw_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "_h2ZKq5Tctw_",
    "outputId": "36d9aa00-cced-4be9-f83a-949602b67ea5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/MyDrive/YDS/AIConnet_YDS_1/NLP_MRC/results/train/20221118_171617'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RECORDER_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e2ce2-c11a-42b0-baaa-107251e077a3",
   "metadata": {
    "id": "ea6e2ce2-c11a-42b0-baaa-107251e077a3"
   },
   "source": [
    "###7.3 추론 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6876e75d-f3ba-4007-a548-062d4b1ce681",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6876e75d-f3ba-4007-a548-062d4b1ce681",
    "outputId": "0d6c4f66-8327-43ff-e8a7-67ffa835cff7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:08<00:00,  8.35it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()     # 모델을 eval mode로 전환. train mode와 달리 eval mode에서는 dropout, batchnorm이 적용되지 않는다\n",
    "\n",
    "pred_df = pd.read_csv(os.path.join(DATA_DIR, 'sample_submission.csv'))\n",
    "\n",
    "for batch_index, batch in enumerate(tqdm(test_dataloader, leave=True)):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "    # Inference\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    start_score = outputs.start_logits\n",
    "    end_score = outputs.end_logits\n",
    "\n",
    "    start_idx = torch.argmax(start_score, dim=1).cpu().tolist()\n",
    "    end_idx = torch.argmax(end_score, dim=1).cpu().tolist()\n",
    "\n",
    "    y_pred = []\n",
    "    for i in range(len(input_ids)):\n",
    "        if start_idx[i] > end_idx[i]:\n",
    "            output = ''\n",
    "\n",
    "        ans_txt = tokenizer.decode(input_ids[i][start_idx[i]:end_idx[i]]).replace('#','')\n",
    "\n",
    "        if ans_txt == '[CLS]':\n",
    "            ans_txt == ''\n",
    "\n",
    "        y_pred.append(ans_txt)\n",
    "\n",
    "\n",
    "    q_end_idx = BATCH_SIZE*batch_index + len(y_pred)\n",
    "    for q_id, pred in zip(question_ids[BATCH_SIZE*batch_index:q_end_idx], y_pred):\n",
    "        pred_df.loc[pred_df['question_id'] == q_id,'answer_text'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "kJ1FASxgU1IS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "kJ1FASxgU1IS",
    "outputId": "96006982-a938-4888-c39c-0d677104b590"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-f3074fa7-b378-41af-bc1a-02c726db7651\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QUES_cyOI2451l1</td>\n",
       "      <td>한국원자력안전기술원</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QUES_pz2vbWpWWo</td>\n",
       "      <td>가출청소년 문제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QUES_1g3jI4y7eo</td>\n",
       "      <td>탐색기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QUES_qzwOZwaeeY</td>\n",
       "      <td>Prime Air</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QUES_hfdtXCtdzf</td>\n",
       "      <td>장애인케어서비스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>QUES_JtsKBSQITG</td>\n",
       "      <td>상용직</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>QUES_IajaDLmxvq</td>\n",
       "      <td>처분가능소득</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>QUES_lR6hjzsptY</td>\n",
       "      <td>대체보육서비스 비용</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>QUES_ACwZJGYBfp</td>\n",
       "      <td>IBM Food Trust Labeyrie사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>QUES_UB3Fj0NPIX</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1626 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f3074fa7-b378-41af-bc1a-02c726db7651')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-f3074fa7-b378-41af-bc1a-02c726db7651 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-f3074fa7-b378-41af-bc1a-02c726db7651');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "          question_id               answer_text\n",
       "0     QUES_cyOI2451l1                한국원자력안전기술원\n",
       "1     QUES_pz2vbWpWWo                  가출청소년 문제\n",
       "2     QUES_1g3jI4y7eo                       탐색기\n",
       "3     QUES_qzwOZwaeeY                 Prime Air\n",
       "4     QUES_hfdtXCtdzf                  장애인케어서비스\n",
       "...               ...                       ...\n",
       "1621  QUES_JtsKBSQITG                       상용직\n",
       "1622  QUES_IajaDLmxvq                    처분가능소득\n",
       "1623  QUES_lR6hjzsptY                대체보육서비스 비용\n",
       "1624  QUES_ACwZJGYBfp  IBM Food Trust Labeyrie사\n",
       "1625  QUES_UB3Fj0NPIX                      2009\n",
       "\n",
       "[1626 rows x 2 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2860d018-7dda-434f-97ef-cd59a0572c92",
   "metadata": {
    "id": "2860d018-7dda-434f-97ef-cd59a0572c92"
   },
   "source": [
    "###7.4 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8f98cd03-5a99-4911-a72e-bc81e49c18be",
   "metadata": {
    "id": "8f98cd03-5a99-4911-a72e-bc81e49c18be"
   },
   "outputs": [],
   "source": [
    "# Set predict serial\n",
    "kst = timezone(timedelta(hours=9))\n",
    "predict_timestamp = datetime.now(tz=kst).strftime(\"%Y%m%d_%H%M%S\")\n",
    "predict_serial = predict_timestamp\n",
    "predict_serial\n",
    "\n",
    "PREDICT_DIR = os.path.join(PROJECT_DIR, 'results', 'predict', predict_serial)\n",
    "os.makedirs(PREDICT_DIR, exist_ok=True)\n",
    "\n",
    "pred_df.to_csv(os.path.join(PREDICT_DIR, 'prediction.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SX6apf0nRz1Y",
   "metadata": {
    "id": "SX6apf0nRz1Y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lhs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "fb92ee058462925f1c2b46ec0ce4bc93b4ae93ba050752c77697e386d6fa9f39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
